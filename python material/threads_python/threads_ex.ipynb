{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Exercise: thread files\n",
    "• Get a list of files (from the current directory or from all the files in the “slides” repository.\n",
    "• Process each file:\n",
    "• 1. get size of file\n",
    "• 2. count how many times each character appear in the file.\n",
    "• The script should accept the number of threads to use\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "files = os.listdir(\"for_test\")\n",
    "for index in range(len(files)):\n",
    "    files[index] = f'for_test/{files[index]}'\n",
    "\n",
    "def count_str(file_path: str):\n",
    "    with open(file_path, 'r') as file:\n",
    "        laters = file.read()\n",
    "        size = os.stat(file_path).st_size\n",
    "        print('name file is ' + file_path.split('/')[-1])\n",
    "        print(f\"{Counter(laters)} times in \" + file_path.split('/')[-1])\n",
    "        print(f'size of file is {size}')\n",
    "\n",
    "\n",
    "def op1():\n",
    "    with concurrent.futures.ThreadPoolExecutor() as ex:\n",
    "        ex.map(count_str, files)\n",
    "\n",
    "def op2():\n",
    "    threads = []\n",
    "    for file in files:\n",
    "        thread = threading.Thread(target=count_str, args=(file,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "def without_threads():\n",
    "    for file in files:\n",
    "        count_str(file)\n",
    "    \n",
    "def main():\n",
    "    start1 = time.perf_counter()\n",
    "    op1()\n",
    "    end1 = time.perf_counter()\n",
    "    ##########\n",
    "    start2 = time.perf_counter()\n",
    "    op2()\n",
    "    end2 = time.perf_counter()\n",
    "    ##########\n",
    "    start3 = time.perf_counter()\n",
    "    without_threads()\n",
    "    end3 = time.perf_counter()\n",
    "    ##########\n",
    "    print(f'with op 1 its take time {round(start1 - end1, 3)}\\n')\n",
    "    print(f'with op 2 its take time {round(start2 - end3, 3)}\\n')\n",
    "    print(f'without threads its take time {round(start3 - end3, 3)}\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Exercise: thread URL requests.\n",
    "In the following script we fetch the URLs listed in a file (urls.txt):\n",
    " Given a file with a list of URLs, collect the title of each site.\n",
    " \n",
    "1 https://google.com/\n",
    "2 https://youtube.com/\n",
    "3 https://facebook.com/\n",
    "4 https://baidu.com/\n",
    "5 https://twitter.com/\n",
    "6 https://instagram.com/\n",
    "7 https://wikipedia.com/\n",
    "8 https://www.amazon.com/\n",
    "9 https://yahoo.com/\n",
    "10 https://yandex.ru/\n",
    "11 https://vk.com/\n",
    "12 https://live.com/\n",
    "13 https://naver.com/\n",
    "14 https://yahoo.co.jp/\n",
    "15 https://google.com.br/\n",
    "16 https://netflix.com/\n",
    "17 https://reddit.com/\n",
    "18 https://ok.ru/\n",
    "19 https://mail.ru/\n",
    "20 https://ebay.com/\n",
    "21 https://linkedin.com/\n",
    "22 https://qq.com/\n",
    "23 https://pinterest.com/\n",
    "24 https://bing.com/\n",
    "25 https://whatsapp.com/\n",
    "26 https://office.com/\n",
    "27 https://amazon.de/\n",
    "28 https://aliexpress.com/\n",
    "29 https://amazon.co.jp/\n",
    "30 https://msn.com/\n",
    "31 https://google.de/\n",
    "32 https://paypal.com/\n",
    "33 https://rakuten.co.jp/\n",
    "34 https://amazon.co.uk/\n",
    "35 https://daum.net/\n",
    "36 https://google.co.jp/\n",
    "37 https://taobao.com/\n",
    "38 https://bilbili.com/\n",
    "39 https://imdb.com/\n",
    "40 https://booking.com/\n",
    "41 https://roblox.com/\n",
    "42 https://9apps.com/\n",
    "43 https://globo.com/Threads 858\n",
    "44 https://duckduckgo.com/\n",
    "45 https://www.nttdocomo.co.jp/\n",
    "\n",
    "It takes about 1.5-2 sec / URL from home. \n",
    "(It depends on a lot of factors including your network connection.)\n",
    "\n",
    "Create a version of the above script that can use K threads\n",
    "\"\"\"\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_urls(limit):\n",
    "    with open('urls.txt') as fh:\n",
    "        urls = list(map(lambda line: line.rstrip(\"\\n\"), fh))\n",
    "    print(urls)\n",
    "    if len(urls) > limit:\n",
    "        urls = urls[:limit]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_title(url):\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code != 200:\n",
    "            return None, f\"Incorrect status_code {resp.status_code} for {url}\"\n",
    "    except Exception as err:\n",
    "        return None, f\"Error: {err} for {url}\"\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    return soup.title.string, None\n",
    "\n",
    "\n",
    "def get_titles(urls):\n",
    "    titles = []\n",
    "    for url in urls:\n",
    "        # print(f\"Processing {url}\")\n",
    "        title, err = get_title(url)\n",
    "        if err:\n",
    "            print(err)\n",
    "        else:\n",
    "            print(title)\n",
    "        titles.append({\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"err\": err,\n",
    "        })\n",
    "    return titles\n",
    "\n",
    "\n",
    "def main():\n",
    "    # if len(sys.argv) < 2:\n",
    "    #     exit(f\"Usage: {sys.argv[0]} LIMIT\")\n",
    "    # limit = int(sys.argv[1])\n",
    "    limit = 40\n",
    "    urls = get_urls(limit)\n",
    "    # print(urls)\n",
    "    start = time.time()\n",
    "\n",
    "    titles = get_titles(urls)\n",
    "    end = time.time()\n",
    "    print(f\"Elapsed time: {end - start} for {len(urls)} pages.\")\n",
    "    # print(titles)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multi proses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import multiprocessing as mp\n",
    "import \n",
    "\n",
    "\n",
    "def get_urls(limit):\n",
    "    with open('urls.txt') as fh:\n",
    "        urls = list(map(lambda line: line.rstrip(\"\\n\"), fh))\n",
    "    # print(urls)\n",
    "    if len(urls) > limit:\n",
    "        urls = urls[:limit]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_title(url):\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code != 200:\n",
    "\n",
    "            return url, None, f\"Incorrect status_code {resp.status_code} for {url}\"\n",
    "    except Exception as err:\n",
    "        return url, None, f\"Error: {err} for {url}\"\n",
    "    # soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    # print(soup.title.string)\n",
    "    return url, resp.content[:10], None\n",
    "\n",
    "\n",
    "def get_titles(urls):\n",
    "    pool = mp.Pool()\n",
    "    results = pool.map(get_title, urls)\n",
    "    titles = []\n",
    "    for result in results:\n",
    "        url, title, arr = result\n",
    "        if arr:\n",
    "            print(arr)\n",
    "        else:\n",
    "            print(title)\n",
    "            titles.append({'url': url, 'title': title, 'arr': arr})\n",
    "    return titles\n",
    "\n",
    "\n",
    "def main():\n",
    "    # if len(sys.argv) < 2:\n",
    "    #     exit(f\"Usage: {sys.argv[0]} LIMIT\")\n",
    "    # limit = int(sys.argv[1])\n",
    "    limit = 40\n",
    "    urls = get_urls(limit)\n",
    "    # print(urls)\n",
    "    start = time.time()\n",
    "    titles = get_titles(urls)\n",
    "    end = time.time()\n",
    "    print(f\"Elapsed time: {end - start} for {len(urls)} pages.\")\n",
    "    print(titles)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multi threads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "\n",
    "\n",
    "def get_urls(limit):\n",
    "    with open('urls.txt') as fh:\n",
    "        urls = list(map(lambda line: line.rstrip(\"\\n\"), fh))\n",
    "    # print(urls)\n",
    "    if len(urls) > limit:\n",
    "        urls = urls[:limit]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_title(url):\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code != 200:\n",
    "            return url, None, f\"Incorrect status_code {resp.status_code} for {url}\"\n",
    "    except Exception as err:\n",
    "        return url, None, f\"Error: {err} for {url}\"\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    return url, soup.title.string, None\n",
    "\n",
    "\n",
    "def get_titles(urls):\n",
    "    results = []\n",
    "    threads = []\n",
    "    for url in urls:\n",
    "        thread = threading.Thread(\n",
    "            target=lambda: results.append(get_title(url)))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    titles = []\n",
    "    for result in results:\n",
    "        url, title, arr = result\n",
    "        if arr:\n",
    "            print(f\"url is {url}, arr is {arr}\")\n",
    "        else:\n",
    "            print(f\"url is {url}, title is {title}\")\n",
    "        titles.append({'url': url, 'title': title, 'arr': arr})\n",
    "    return titles\n",
    "\n",
    "\n",
    "def main():\n",
    "    limit = 40\n",
    "    urls = get_urls(limit)\n",
    "    print(urls)\n",
    "    start = time.time()\n",
    "    titles = get_titles(urls)\n",
    "    end = time.time()\n",
    "    print(f\"Elapsed time: {end - start} for {len(urls)} pages.\")\n",
    "    print(titles)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Exercise: thread queue\n",
    "Write an application that handles a queue of jobs in N=5 threads.\n",
    "Each job contains a number between 0-5.\n",
    "Each thread takes the next element from the queue and sleeps for the given amount\n",
    "of second (as an imitation of actual work it should be doing). When finished it checks\n",
    "for another job. If there are no more jobs in the queue, the thread can close itself.\n",
    "\n",
    "If that’s done, change the code so that each thread will generate a random\n",
    "number between 0-5 (for sleep-time) and in 33% of the cases it will add it to the central queue\n",
    "as a new job.\n",
    "\n",
    "Another extension to this exercise is to change the code to limit the number of jobs each thread\n",
    "can execute in its lifetime. When the thread has finished that many jobs it will quit and the\n",
    "main thread will create a new worker thread.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "thread_count = 5\n",
    "counter = 0\n",
    "queue = list(map(lambda x: ('main', random.randrange(5)), range(20)))\n",
    "\n",
    "def performing_task():\n",
    "    while queue:\n",
    "        print(len(queue))\n",
    "        task = queue.pop()\n",
    "        time.sleep(task[1])\n",
    "\n",
    "def ex3():\n",
    "    threads = []\n",
    "    for number in range(thread_count):\n",
    "        thread = threading.Thread(target=performing_task, name=f'number thread {number}')\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    \n",
    "ex3()\n",
    "print(queue)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
