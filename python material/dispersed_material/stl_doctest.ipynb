{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doctest — Testing Through Documentation\n",
    "\n",
    "**Purpose**:\tWrite automated tests as part of the documentation for a module.\n",
    "\n",
    "`doctest` tests source code by running examples embedded in the documentation and verifying that they produce the expected results. \n",
    "\n",
    "It works by parsing the help text to find examples, running them, then comparing the output text against the expected value. \n",
    "\n",
    "Many developers find `doctest` easier to use than `unittest` because, in its simplest form, there is no API to learn before using it. \n",
    "\n",
    "However, as the examples become more complex the lack of fixture management can make writing `doctest` tests more cumbersome than using `unittest`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vadim\\\\Documents\\\\GitHub\\\\Python\\\\_Lessons_\\\\MEFATHIM\\\\Sylllabus-AdvancedPython\\\\week4\\\\exercises\\\\doctest'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import doctest\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Vadim\\Documents\\GitHub\\Python\\_Lessons_\\MEFATHIM\\Sylllabus-AdvancedPython\\week4\\exercises\\doctest\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    my_function(2, 3)\n",
      "Expecting:\n",
      "    6\n",
      "ok\n",
      "Trying:\n",
      "    my_function('a', 3)\n",
      "Expecting:\n",
      "    'aaa'\n",
      "ok\n",
      "1 items had no tests:\n",
      "    doctest_simple\n",
      "1 items passed all tests:\n",
      "   2 tests in doctest_simple.my_function\n",
      "2 tests in 2 items.\n",
      "2 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "!python -m doctest -v doctest_simple.py\n",
    "\n",
    "# To run the tests, use doctest as the main program via the -m option. \n",
    "# Usually no output is produced while the tests are running, so the next example includes the -v option to make the output more verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    unpredictable(MyClass()) #doctest: +ELLIPSIS\n",
      "Expecting:\n",
      "    [<doctest_ellipsis.MyClass object at 0x...>]\n",
      "ok\n",
      "2 items had no tests:\n",
      "    doctest_ellipsis\n",
      "    doctest_ellipsis.MyClass\n",
      "1 items passed all tests:\n",
      "   1 tests in doctest_ellipsis.unpredictable\n",
      "1 tests in 3 items.\n",
      "1 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"When the tests include values that are likely to change in unpredictable ways, and where the actual value is not important to the test results, \n",
    "use the ELLIPSIS option to tell doctest to ignore portions of the verification value.\n",
    "The “#doctest: +ELLIPSIS” comment after the call to unpredictable() tells doctest to turn on the ELLIPSIS option for that test. \n",
    "The ... replaces the memory address in the object id, so that portion of the expected value is ignored and the actual output matches and the test passes.\n",
    "\"\"\"\n",
    "\n",
    "!python -m doctest -v doctest_ellipsis.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Locations\n",
    "\n",
    "All of the tests in the examples so far have been written in the docstrings of the functions they are testing. \n",
    "\n",
    "That is convenient for users who examine the docstrings for help using the function (especially with `pydoc`), but doctest looks for tests in other places, too. \n",
    "\n",
    "The obvious location for additional tests is in the docstrings elsewhere in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    A('a') == B('b')\n",
      "Expecting:\n",
      "    False\n",
      "ok\n",
      "Trying:\n",
      "    A('instance_name').name\n",
      "Expecting:\n",
      "    'instance_name'\n",
      "ok\n",
      "Trying:\n",
      "    A('name').method()\n",
      "Expecting:\n",
      "    'eman'\n",
      "ok\n",
      "Trying:\n",
      "    B('different_name').name\n",
      "Expecting:\n",
      "    'different_name'\n",
      "ok\n",
      "1 items had no tests:\n",
      "    doctest_docstrings.A.__init__\n",
      "4 items passed all tests:\n",
      "   1 tests in doctest_docstrings\n",
      "   1 tests in doctest_docstrings.A\n",
      "   1 tests in doctest_docstrings.A.method\n",
      "   1 tests in doctest_docstrings.B\n",
      "4 tests in 5 items.\n",
      "4 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "!python -m doctest -v doctest_docstrings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    my_function(2, 3)\n",
      "Expecting:\n",
      "    6\n",
      "ok\n",
      "Trying:\n",
      "    my_function('a', 3)\n",
      "Expecting:\n",
      "    'aaa'\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "def my_function(a, b):\n",
    "    \"\"\"\n",
    "    >>> my_function(2, 3)\n",
    "    6\n",
    "    >>> my_function('a', 3)\n",
    "    'aaa'\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "doctest.run_docstring_examples(my_function, globals(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    add(7,6)\n",
      "Expecting:\n",
      "    13\n",
      "ok\n",
      "1 items had no tests:\n",
      "    __main__\n",
      "1 items passed all tests:\n",
      "   1 tests in __main__.add\n",
      "1 tests in 2 items.\n",
      "1 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    '''\n",
    "    This is a test:\n",
    "    >>> add(7,6)\n",
    "    13\n",
    "    '''\n",
    "    return a + b\n",
    "\n",
    "doctest.testmod(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
